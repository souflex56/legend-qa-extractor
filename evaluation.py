#!/usr/bin/env python3
"""
è¯„ä¼°è„šæœ¬ - ç¬¬äºŒé˜¶æ®µï¼šå»ºç«‹é‡åŒ–è¯„ä¼°ä½“ç³»
ç”¨äºå®¢è§‚è¯„ä¼° legend-qa-extractor ç³»ç»Ÿçš„è¡¨ç°

ä¾èµ–å®‰è£…ï¼š
pip install sentence-transformers

ä½¿ç”¨æ–¹æ³•ï¼š
1. å…ˆåˆ›å»ºé»„é‡‘æ ‡å‡†é›† golden_set.jsonlï¼ˆäººå·¥æ ‡æ³¨ï¼‰
2. è¿è¡Œä¸»æµç¨‹ç”Ÿæˆ extracted_qa_smart_v1.jsonl
3. è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œè¯„ä¼°ï¼špython evaluation.py
"""

import json
import os
import logging
from typing import List, Dict, Any, Tuple
import numpy as np

try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError as e:
    print(f"ä¾èµ–åº“ç¼ºå¤±ï¼Œè¯·å®‰è£…ï¼špip install sentence-transformers scikit-learn")
    print(f"é”™è¯¯è¯¦æƒ…ï¼š{e}")
    exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class QAEvaluator:
    """QAæå–ç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, 
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 golden_set_path: str = "golden_set.jsonl",
                 generated_qa_path: str = "output/extracted_qa_smart_v1.jsonl"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_name: å¥å­åµŒå…¥æ¨¡å‹åç§°
            golden_set_path: é»„é‡‘æ ‡å‡†é›†æ–‡ä»¶è·¯å¾„
            generated_qa_path: ç”Ÿæˆçš„QAæ–‡ä»¶è·¯å¾„
        """
        self.golden_set_path = golden_set_path
        self.generated_qa_path = generated_qa_path
        
        logger.info(f"åŠ è½½å¥å­åµŒå…¥æ¨¡å‹: {model_name}")
        self.embedding_model = SentenceTransformer(model_name)
        
        self.golden_qa_pairs = []
        self.generated_qa_pairs = []
    
    def load_qa_pairs(self, file_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½QAå¯¹æ•°æ®"""
        qa_pairs = []
        
        if not os.path.exists(file_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return qa_pairs
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        qa_pair = json.loads(line.strip())
                        if self._is_valid_qa_pair(qa_pair):
                            qa_pairs.append(qa_pair)
                        else:
                            logger.warning(f"ç¬¬{line_num}è¡ŒQAå¯¹æ ¼å¼æ— æ•ˆ: {line.strip()[:100]}...")
                    except json.JSONDecodeError as e:
                        logger.warning(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
            
            logger.info(f"ä» {file_path} åŠ è½½äº† {len(qa_pairs)} ä¸ªæœ‰æ•ˆQAå¯¹")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        return qa_pairs
    
    def _is_valid_qa_pair(self, qa_pair: Dict[str, Any]) -> bool:
        """éªŒè¯QAå¯¹æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
        return (isinstance(qa_pair, dict) and 
                "question" in qa_pair and "answer" in qa_pair and
                qa_pair.get("question") and qa_pair.get("answer") and
                isinstance(qa_pair["question"], str) and 
                isinstance(qa_pair["answer"], str))
    
    def find_most_similar_question(self, target_question: str, candidate_qa_pairs: List[Dict[str, Any]]) -> Tuple[int, float]:
        """
        åœ¨å€™é€‰QAå¯¹ä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„é—®é¢˜
        
        Args:
            target_question: ç›®æ ‡é—®é¢˜
            candidate_qa_pairs: å€™é€‰QAå¯¹åˆ—è¡¨
            
        Returns:
            (best_match_index, similarity_score)
        """
        if not candidate_qa_pairs:
            return -1, 0.0
        
        candidate_questions = [qa["question"] for qa in candidate_qa_pairs]
        
        # è®¡ç®—åµŒå…¥å‘é‡
        target_embedding = self.embedding_model.encode([target_question])
        candidate_embeddings = self.embedding_model.encode(candidate_questions)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(target_embedding, candidate_embeddings)[0]
        
        # æ‰¾åˆ°æœ€é«˜ç›¸ä¼¼åº¦
        best_index = np.argmax(similarities)
        best_similarity = similarities[best_index]
        
        return best_index, best_similarity
    
    def calculate_answer_similarity(self, answer1: str, answer2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªç­”æ¡ˆä¹‹é—´çš„ç›¸ä¼¼åº¦"""
        embeddings = self.embedding_model.encode([answer1, answer2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        return similarity
    
    def evaluate_qa_extraction(self) -> Dict[str, Any]:
        """
        ä¸»è¯„ä¼°å‡½æ•°ï¼šå¯¹æ¯”ç”Ÿæˆçš„QAå¯¹ä¸é»„é‡‘æ ‡å‡†é›†
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹QAæå–è´¨é‡è¯„ä¼°...")
        
        # åŠ è½½æ•°æ®
        self.golden_qa_pairs = self.load_qa_pairs(self.golden_set_path)
        self.generated_qa_pairs = self.load_qa_pairs(self.generated_qa_path)
        
        if not self.golden_qa_pairs:
            logger.error("é»„é‡‘æ ‡å‡†é›†ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return {"error": "é»„é‡‘æ ‡å‡†é›†ä¸ºç©º"}
        
        if not self.generated_qa_pairs:
            logger.error("ç”Ÿæˆçš„QAå¯¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
            return {"error": "ç”Ÿæˆçš„QAå¯¹ä¸ºç©º"}
        
        # è¯„ä¼°æŒ‡æ ‡
        evaluation_results = {
            "golden_set_size": len(self.golden_qa_pairs),
            "generated_set_size": len(self.generated_qa_pairs),
            "question_similarities": [],
            "answer_similarities": [],
            "matched_pairs": 0,
            "average_question_similarity": 0.0,
            "average_answer_similarity": 0.0,
            "overall_score": 0.0,
            "detailed_matches": []
        }
        
        logger.info(f"é»„é‡‘æ ‡å‡†é›†å¤§å°: {len(self.golden_qa_pairs)}")
        logger.info(f"ç”Ÿæˆé›†å¤§å°: {len(self.generated_qa_pairs)}")
        
        # å¯¹æ¯ä¸ªé»„é‡‘æ ‡å‡†QAå¯¹ï¼Œåœ¨ç”Ÿæˆé›†ä¸­æ‰¾æœ€ç›¸ä¼¼çš„
        question_similarities = []
        answer_similarities = []
        
        for i, golden_qa in enumerate(self.golden_qa_pairs):
            golden_question = golden_qa["question"]
            golden_answer = golden_qa["answer"]
            
            # åœ¨ç”Ÿæˆé›†ä¸­æ‰¾æœ€ç›¸ä¼¼çš„é—®é¢˜
            best_match_idx, question_sim = self.find_most_similar_question(
                golden_question, self.generated_qa_pairs
            )
            
            if best_match_idx >= 0:
                matched_generated_qa = self.generated_qa_pairs[best_match_idx]
                generated_answer = matched_generated_qa["answer"]
                
                # è®¡ç®—ç­”æ¡ˆç›¸ä¼¼åº¦
                answer_sim = self.calculate_answer_similarity(golden_answer, generated_answer)
                
                question_similarities.append(question_sim)
                answer_similarities.append(answer_sim)
                
                # å­˜å‚¨è¯¦ç»†åŒ¹é…ä¿¡æ¯
                evaluation_results["detailed_matches"].append({
                    "golden_index": i,
                    "matched_generated_index": best_match_idx,
                    "question_similarity": float(question_sim),
                    "answer_similarity": float(answer_sim),
                    "golden_question": golden_question[:100] + "..." if len(golden_question) > 100 else golden_question,
                    "matched_question": matched_generated_qa["question"][:100] + "..." if len(matched_generated_qa["question"]) > 100 else matched_generated_qa["question"],
                    "golden_answer": golden_answer[:100] + "..." if len(golden_answer) > 100 else golden_answer,
                    "matched_answer": generated_answer[:100] + "..." if len(generated_answer) > 100 else generated_answer
                })
                
                logger.debug(f"é»„é‡‘æ ‡å‡† {i+1}: é—®é¢˜ç›¸ä¼¼åº¦={question_sim:.3f}, ç­”æ¡ˆç›¸ä¼¼åº¦={answer_sim:.3f}")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        if question_similarities:
            evaluation_results["question_similarities"] = question_similarities
            evaluation_results["answer_similarities"] = answer_similarities
            evaluation_results["matched_pairs"] = len(question_similarities)
            evaluation_results["average_question_similarity"] = float(np.mean(question_similarities))
            evaluation_results["average_answer_similarity"] = float(np.mean(answer_similarities))
            
            # ç»¼åˆåˆ†æ•°ï¼šé—®é¢˜å’Œç­”æ¡ˆç›¸ä¼¼åº¦çš„åŠ æƒå¹³å‡
            evaluation_results["overall_score"] = float(
                0.3 * evaluation_results["average_question_similarity"] + 
                0.7 * evaluation_results["average_answer_similarity"]
            )
        
        return evaluation_results
    
    def generate_evaluation_report(self, results: Dict[str, Any], output_path: str = "evaluation_report.json"):
        """ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
        
        # ä¿å­˜å®Œæ•´ç»“æœåˆ°JSONæ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦æŠ¥å‘Š
        print("\n" + "="*80)
        print("QAæå–ç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š")
        print("="*80)
        
        if "error" in results:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {results['error']}")
            return
        
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   é»„é‡‘æ ‡å‡†é›†å¤§å°: {results['golden_set_size']}")
        print(f"   ç”Ÿæˆé›†å¤§å°: {results['generated_set_size']}")
        print(f"   æˆåŠŸåŒ¹é…å¯¹æ•°: {results['matched_pairs']}")
        
        print(f"\nğŸ“ˆ ç›¸ä¼¼åº¦æŒ‡æ ‡:")
        print(f"   å¹³å‡é—®é¢˜ç›¸ä¼¼åº¦: {results['average_question_similarity']:.4f}")
        print(f"   å¹³å‡ç­”æ¡ˆç›¸ä¼¼åº¦: {results['average_answer_similarity']:.4f}")
        print(f"   ç»¼åˆè¯„åˆ†: {results['overall_score']:.4f}")
        
        # è¯„åˆ†ç­‰çº§
        score = results['overall_score']
        if score >= 0.9:
            grade = "ä¼˜ç§€ ğŸ†"
        elif score >= 0.8:
            grade = "è‰¯å¥½ ğŸ‘"
        elif score >= 0.7:
            grade = "ä¸€èˆ¬ ğŸ‘Œ"
        elif score >= 0.6:
            grade = "åŠæ ¼ âš ï¸"
        else:
            grade = "éœ€è¦æ”¹è¿› âŒ"
        
        print(f"   ç³»ç»Ÿè¯„çº§: {grade}")
        
        print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")
        print("="*80)
    
    def create_sample_golden_set(self, sample_path: str = "golden_set_sample.jsonl"):
        """åˆ›å»ºç¤ºä¾‹é»„é‡‘æ ‡å‡†é›†æ–‡ä»¶ï¼ˆä¾›å‚è€ƒï¼‰"""
        sample_data = [
            {
                "question": "ä»€ä¹ˆæ˜¯stop doing listï¼Ÿ",
                "answer": "æ‰€è°“è¦åšå¯¹çš„äº‹æƒ…å®é™…ä¸Šæ˜¯é€šè¿‡ä¸åšä¸å¯¹çš„äº‹æƒ…æ¥å®ç°çš„ã€‚"
            },
            {
                "question": "ä»·å€¼æŠ•èµ„çš„æ ¸å¿ƒæ˜¯ä»€ä¹ˆï¼Ÿ",
                "answer": "ä»·å€¼æŠ•èµ„çš„æ ¸å¿ƒå°±æ˜¯ä¹°è‚¡ç¥¨å°±æ˜¯ä¹°å…¬å¸ï¼Œä¹°å…¬å¸å°±æ˜¯ä¹°è¿™å®¶å…¬å¸çš„ç”Ÿæ„ã€‚"
            },
            {
                "question": "å¦‚ä½•çœ‹å¾…å¸‚åœºæ³¢åŠ¨ï¼Ÿ",
                "answer": "å¸‚åœºå…ˆç”Ÿçš„æŠ¥ä»·æ¯å¤©éƒ½ä¸ä¸€æ ·ï¼Œä½ è¦æŠŠä»–å½“æˆä¸€ä¸ªèºéƒç—‡æ‚£è€…ï¼Œä¸è¦å¤ªæŠŠä»–çš„è¯å½“çœŸã€‚"
            }
        ]
        
        with open(sample_path, 'w', encoding='utf-8') as f:
            for item in sample_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        logger.info(f"ç¤ºä¾‹é»„é‡‘æ ‡å‡†é›†å·²åˆ›å»º: {sample_path}")
        print(f"ğŸ“ ç¤ºä¾‹é»„é‡‘æ ‡å‡†é›†å·²åˆ›å»º: {sample_path}")
        print("è¯·æ ¹æ®æ‚¨çš„å®é™…éœ€æ±‚ï¼Œåˆ›å»ºåŒ…å«30-50ä¸ªé«˜è´¨é‡QAå¯¹çš„ golden_set.jsonl æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    evaluator = QAEvaluator()
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é»„é‡‘æ ‡å‡†é›†
    if not os.path.exists(evaluator.golden_set_path):
        print(f"âŒ é»„é‡‘æ ‡å‡†é›†æ–‡ä»¶ä¸å­˜åœ¨: {evaluator.golden_set_path}")
        print("æ­£åœ¨åˆ›å»ºç¤ºä¾‹æ–‡ä»¶...")
        evaluator.create_sample_golden_set()
        return
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç”Ÿæˆçš„QAæ–‡ä»¶
    if not os.path.exists(evaluator.generated_qa_path):
        print(f"âŒ ç”Ÿæˆçš„QAæ–‡ä»¶ä¸å­˜åœ¨: {evaluator.generated_qa_path}")
        print("è¯·å…ˆè¿è¡Œä¸»æµç¨‹ç”ŸæˆQAæ–‡ä»¶")
        return
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate_qa_extraction()
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluator.generate_evaluation_report(results)


if __name__ == "__main__":
    main() 