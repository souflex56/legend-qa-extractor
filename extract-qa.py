#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from typing import Optional, List
import json
import re
import fitz  # PyMuPDF
import logging
import random
import os
from tqdm import tqdm
from ollama import Client

# ==================== é…ç½®åŒº ====================
# è·å–è„šæœ¬æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# âœ… æ–‡ä»¶åé…ç½® - åªéœ€ä¿®æ”¹æ–‡ä»¶åå³å¯
PDF_FILENAME = "uploaded.pdf"
OUTPUT_FILENAME = "output_final_duan-qa.jsonl"

# âœ… è‡ªåŠ¨æ‹¼æ¥å®Œæ•´è·¯å¾„
PDF_PATH = os.path.join(SCRIPT_DIR, PDF_FILENAME)

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "output")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# æ‰€æœ‰è¾“å‡ºæ–‡ä»¶éƒ½æ”¾åœ¨outputæ–‡ä»¶å¤¹ä¸­
OUTPUT_PATH = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)
ERROR_LOG_PATH = os.path.join(OUTPUT_DIR, "extraction_errors_final.log")
SUCCESS_LOG_PATH = os.path.join(OUTPUT_DIR, "extraction_success_final.log")

# âœ… æ¨¡å‹é…ç½® - å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶
MODEL_NAME = os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct")

MAX_BLOCK_SIZE = 1500
MIN_BLOCK_SIZE = 100
# âœ… æ–‡æœ¬æå–æ¯”ä¾‹ï¼Œå¯è‡ªå®šä¹‰ï¼Œ0.03 è¡¨ç¤ºé¡ºåºæå–å‰ 3% çš„å—ï¼Œ1 è¡¨ç¤ºæå–æ‰€æœ‰å—
EXTRACT_RATIO = 1

# âœ… æ˜¯å¦å¼€å¯é—®ç­”æ£€æµ‹è¿‡æ»¤ï¼ˆTrue = åªå¤„ç†å«é—®ç­”å—ï¼›False = ä¿è¯å®Œæ•´æ€§ï¼Œæ‰€æœ‰å—éƒ½å¤„ç†ï¼‰
ENABLE_QA_FILTER = False
# =================================================

# --- æ—¥å¿—è®¾ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# é”™è¯¯æ—¥å¿—
error_logger = logging.getLogger('error_logger')
error_handler = logging.FileHandler(ERROR_LOG_PATH, mode='w', encoding='utf-8')
error_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - BLOCK_START\n%(message)s\nBLOCK_END'))
error_logger.addHandler(error_handler)

# æˆåŠŸæå–æ—¥å¿—
success_logger = logging.getLogger('success_logger')
success_handler = logging.FileHandler(SUCCESS_LOG_PATH, mode='w', encoding='utf-8')
success_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - QA_PAIR_START\n%(message)s\nQA_PAIR_END'))
success_logger.addHandler(success_handler)

# --- Prompt ä¼˜åŒ– ---
BASE_PROMPT_CHAT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡é—®ç­”å¯¹æå–ä¸“å®¶ã€‚è¯·ä»ç»™å®šçš„åŸæ–‡ä¸­æå–**æ‰€æœ‰**æœ‰æ•ˆçš„é—®ç­”å¯¹ã€‚

ğŸ¯ æ ¸å¿ƒä»»åŠ¡ï¼š
1. è¯†åˆ«æ‰€æœ‰å½¢å¼çš„æé—®æˆ–è¯é¢˜å¼•å…¥
2. åŒ¹é…å¯¹åº”çš„æ®µæ°¸å¹³å›ç­”
3. ç¡®ä¿é—®ç­”é…å¯¹å‡†ç¡®å®Œæ•´

ğŸ“‹ æå–è§„åˆ™ï¼š
1. **é—®é¢˜æ¥æº**ï¼ˆå¤šç§å½¢å¼ï¼‰ï¼š
   - ç›´æ¥æé—®ï¼šç½‘å‹ã€é—®ã€è§‚ä¼—ã€ä¸»æŒäººã€Qç­‰å¼€å¤´
   - æ–‡ç« å¼•ç”¨ï¼šæ–‡ç« å¼•ç”¨ã€å¼•ç”¨ã€æœ‰äººè¯´ç­‰å¼•å‡ºçš„è§‚ç‚¹æˆ–é—®é¢˜
   - é—´æ¥æé—®ï¼šé€šè¿‡æè¿°ã€ä¸¾ä¾‹å¼•å‡ºçš„é—®é¢˜
   - è¯é¢˜è®¨è®ºï¼šä»»ä½•å¼•å‘æ®µæ°¸å¹³å›åº”çš„å†…å®¹

2. **ç­”æ¡ˆæ¥æº**ï¼šä»…é™æ®µæ°¸å¹³ã€æ®µã€å¤§é“çš„å›ç­”ï¼ˆæ’é™¤æ–¹ä¸ˆã€å…¶ä»–äººï¼‰

3. **é…å¯¹åŸåˆ™**ï¼š
   - ä¸€ä¸ªé—®é¢˜/è¯é¢˜å¯¹åº”ä¸€ä¸ªæ®µæ°¸å¹³çš„å›ç­”
   - é—®é¢˜å¯ä»¥æ˜¯ç›´æ¥æé—®ï¼Œä¹Ÿå¯ä»¥æ˜¯å¼•ç”¨ã€æè¿°ç­‰å½¢å¼
   - ä¿æŒé—®é¢˜å’Œç­”æ¡ˆçš„å®Œæ•´æ€§å’Œä¸Šä¸‹æ–‡

4. **è¾“å‡ºæ ¼å¼**ï¼šJSONæ•°ç»„ [{"question": "é—®é¢˜æˆ–è¯é¢˜", "answer": "æ®µæ°¸å¹³çš„å›ç­”"}]

âŒ ä¸è¦æå–çš„å†…å®¹ï¼š
- æ–¹ä¸ˆçš„å›ç­”
- å…¶ä»–ä¸“å®¶ã€å­¦è€…çš„è§‚ç‚¹ï¼ˆé™¤éæ®µæ°¸å¹³å¯¹æ­¤æœ‰å›åº”ï¼‰
- çº¯æè¿°æ€§æ–‡å­—ï¼ˆé™¤éæ®µæ°¸å¹³å¯¹æ­¤æœ‰å›åº”ï¼‰
- ç›®å½•ã€æ ‡é¢˜ç­‰

âœ… æå–ç¤ºä¾‹ï¼š

åŸæ–‡1ï¼ˆç›´æ¥æé—®ï¼‰ï¼š
ç½‘å‹ï¼šä»€ä¹ˆæ˜¯stop doing listï¼Ÿ
æ®µæ°¸å¹³ï¼šæ‰€è°“è¦åšå¯¹çš„äº‹æƒ…å®é™…ä¸Šæ˜¯é€šè¿‡ä¸åšä¸å¯¹çš„äº‹æƒ…æ¥å®ç°çš„ã€‚

åŸæ–‡2ï¼ˆæ–‡ç« å¼•ç”¨å½¢å¼ï¼‰ï¼š
æ–‡ç« å¼•ç”¨ï¼š"å¾®ä¿¡ä¹‹çˆ¶"å¼ å°é¾™å°±æ›¾è¯´ï¼Œä¹”å¸ƒæ–¯æœ€å‰å®³çš„åœ°æ–¹æ˜¯ä»–1ç§’é’Ÿå°±èƒ½å˜æˆå‚»ç“œã€‚
æ®µï¼šæ˜¯ä¸æ˜¯å¼ å°é¾™è¯´çš„ä¸çŸ¥é“ï¼Œä½†è¿™è¯å…¶å®å¾ˆæœ‰é“ç†ã€‚

åŸæ–‡3ï¼ˆæè¿°å¼•å‡ºï¼‰ï¼š
æœ‰äººè®¤ä¸ºä»·å€¼æŠ•èµ„å¾ˆéš¾å­¦ä¼šã€‚
æ®µï¼šä»·å€¼æŠ•èµ„ç¡®å®ä¸å®¹æ˜“ï¼Œä½†æœ‰æ‚Ÿæ€§çš„äººå¯ä»¥æé«˜ã€‚

è¾“å‡ºï¼š
[
  {"question": "ä»€ä¹ˆæ˜¯stop doing listï¼Ÿ", "answer": "æ‰€è°“è¦åšå¯¹çš„äº‹æƒ…å®é™…ä¸Šæ˜¯é€šè¿‡ä¸åšä¸å¯¹çš„äº‹æƒ…æ¥å®ç°çš„ã€‚"},
  {"question": "æ–‡ç« å¼•ç”¨ï¼šå¾®ä¿¡ä¹‹çˆ¶å¼ å°é¾™å°±æ›¾è¯´ï¼Œä¹”å¸ƒæ–¯æœ€å‰å®³çš„åœ°æ–¹æ˜¯ä»–1ç§’é’Ÿå°±èƒ½å˜æˆå‚»ç“œã€‚", "answer": "æ˜¯ä¸æ˜¯å¼ å°é¾™è¯´çš„ä¸çŸ¥é“ï¼Œä½†è¿™è¯å…¶å®å¾ˆæœ‰é“ç†ã€‚"},
  {"question": "æœ‰äººè®¤ä¸ºä»·å€¼æŠ•èµ„å¾ˆéš¾å­¦ä¼šã€‚", "answer": "ä»·å€¼æŠ•èµ„ç¡®å®ä¸å®¹æ˜“ï¼Œä½†æœ‰æ‚Ÿæ€§çš„äººå¯ä»¥æé«˜ã€‚"}
]

ğŸ” è¯·ä»”ç»†åˆ†æä»¥ä¸‹åŸæ–‡ï¼Œè¯†åˆ«æ‰€æœ‰å¼•å‘æ®µæ°¸å¹³å›åº”çš„å†…å®¹ï¼ˆåŒ…æ‹¬ç›´æ¥æé—®ã€æ–‡ç« å¼•ç”¨ã€æè¿°ç­‰ï¼‰ï¼Œå¹¶æå–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„é—®ç­”å¯¹ï¼š
"""

# --- å·²ä¿®æ”¹ï¼šæ›´æ™ºèƒ½çš„åå¤„ç†å‡½æ•°ï¼Œä¼˜å…ˆå·²çŸ¥å‰ç¼€åˆ—è¡¨ï¼Œå†ç”¨é€šç”¨å®½æ¾æ­£åˆ™å…œåº• ---  # å·²ä¿®æ”¹

def clean_question_text(question: str) -> str:  # å·²ä¿®æ”¹
    """å»é™¤é—®é¢˜å¼€å¤´çš„å„ç§å‘é—®è€…å‰ç¼€ï¼ˆå·²çŸ¥åˆ—è¡¨ä¼˜å…ˆï¼ŒæœªçŸ¥å‰ç¼€å†å®½æ¾åŒ¹é…ï¼‰"""  # å·²ä¿®æ”¹
    # å®‰å…¨æ£€æŸ¥ï¼Œé˜²æ­¢Noneå€¼
    if question is None:
        return ""
    
    # ç¡®ä¿questionæ˜¯å­—ç¬¦ä¸²
    question = str(question)
    
    # å·²çŸ¥å¸¸è§å‰ç¼€åˆ—è¡¨  # å·²ä¿®æ”¹
    known_prefixes = ["ç½‘å‹", "è®°è€…", "é—®", "æé—®è€…", "ä¸»æŒäºº", "æ–‡ç« å¼•ç”¨", "Q", "è§‚ä¼—", "è¯„è®º", "ä¸»æŒ", "ç”¨æˆ·"]  # å·²ä¿®æ”¹
    pattern = r'^({})[ï¼š:]\s*'.format('|'.join(re.escape(p) for p in known_prefixes))  # å·²ä¿®æ”¹
    cleaned = re.sub(pattern, '', question).strip()  # å·²ä¿®æ”¹
    if cleaned != question:  # å·²ä¿®æ”¹
        return cleaned  # å·²ä¿®æ”¹

    # å®½æ¾å…œåº•æ­£åˆ™ï¼ŒåŒ¹é…ä»»ä½•çŸ­å‰ç¼€ + å†’å·  # å·²ä¿®æ”¹
    fallback_pattern = r'^[\u4e00-\u9fa5A-Za-z0-9ï¼ˆï¼‰ã€ã€‘ã€Œã€ã€Šã€‹''""ã€,ï¼Œ.ã€‚Â·\s]{1,20}[ï¼š:]\s*'  # å·²ä¿®æ”¹
    return re.sub(fallback_pattern, '', question).strip()  # å·²ä¿®æ”¹

# --- PDF æå– ---
def extract_text_from_pdf(pdf_path: str) -> str:
    doc = fitz.open(pdf_path)
    all_paragraphs = []
    for page in doc:
        blocks = page.get_text("blocks")
        for b in blocks:
            paragraph_text = b[4].replace('\n', ' ').strip()
            if paragraph_text:
                all_paragraphs.append(paragraph_text)
    return "\n\n".join(all_paragraphs)

# --- æ”¹è¿›åçš„ QA åˆ¤æ–­ ---
def block_has_qa(text: str) -> bool:
    """æ›´ç²¾ç¡®çš„é—®ç­”æ£€æµ‹ï¼ŒåŒ…æ‹¬å¤šç§é—®é¢˜å½¢å¼"""
    # ç›´æ¥æé—®æ¨¡å¼
    direct_question_patterns = [
        r"ç½‘å‹[ï¼š:]",
        r"é—®[ï¼š:]",
        r"é—®é¢˜[ï¼š:]", 
        r"æé—®[ï¼š:]",
        r"ä¸»æŒäºº[ï¼š:]",
        r"è§‚ä¼—[ï¼š:]",
        r"Q[ï¼š:]"
    ]
    
    # é—´æ¥é—®é¢˜æ¨¡å¼ï¼ˆæ–‡ç« å¼•ç”¨ã€æè¿°ç­‰ï¼‰
    indirect_question_patterns = [
        r"æ–‡ç« å¼•ç”¨[ï¼š:]",
        r"å¼•ç”¨[ï¼š:]",
        r"æœ‰äººè¯´",
        r"æœ‰äººè®¤ä¸º",
        r"æœ‰è§‚ç‚¹è®¤ä¸º",
        r"æ®è¯´",
        r"å¬è¯´"
    ]
    
    # æ®µæ°¸å¹³çš„å›ç­”
    answer_patterns = [
        r"æ®µæ°¸å¹³[ï¼š:]",
        r"æ®µ[ï¼š:]",
        r"å¤§é“[ï¼š:]"
    ]
    
    has_direct_question = any(re.search(p, text) for p in direct_question_patterns)
    has_indirect_question = any(re.search(p, text) for p in indirect_question_patterns)
    has_answer = any(re.search(p, text) for p in answer_patterns)
    
    # å¦‚æœæœ‰æ®µæ°¸å¹³çš„å›ç­”ï¼Œä¸”æœ‰ä»»ä½•å½¢å¼çš„é—®é¢˜å¼•å…¥ï¼Œå°±è®¤ä¸ºåŒ…å«é—®ç­”
    return bool(has_answer and (has_direct_question or has_indirect_question))

# --- é¢„å¤„ç†é—®ç­”å¯¹è¯†åˆ« ---
def preprocess_qa_text(text: str) -> str:
    """
    é¢„å¤„ç†æ–‡æœ¬ï¼Œæ ‡å‡†åŒ–é—®ç­”æ ¼å¼ï¼Œä¾¿äºæ¨¡å‹è¯†åˆ«
    """
    # æ ‡å‡†åŒ–é—®ç­”æ ‡è¯†ç¬¦
    text = re.sub(r'ç½‘å‹[ï¼š:]', 'ç½‘å‹ï¼š', text)
    text = re.sub(r'é—®[ï¼š:]', 'é—®ï¼š', text)
    text = re.sub(r'æ®µæ°¸å¹³[ï¼š:]', 'æ®µæ°¸å¹³ï¼š', text)
    text = re.sub(r'æ®µ[ï¼š:]', 'æ®µï¼š', text)
    text = re.sub(r'å¤§é“[ï¼š:]', 'å¤§é“ï¼š', text)
    
    # æ¸…ç†å¤šä½™ç©ºè¡Œ
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    
    return text

# --- æ··åˆåˆ†å— ---
def create_hybrid_blocks(text: str, max_size: int, min_size: int) -> List[str]:
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    all_blocks = []
    i = 0

    while i < len(paragraphs):
        block_paras = []
        current_len = 0

        while i < len(paragraphs):
            para = paragraphs[i]
            block_paras.append(para)
            current_len += len(para)

            if current_len >= max_size:
                i += 1
                break
            i += 1

        block_text = "\n\n".join(block_paras)
        if len(block_text) > min_size:
            all_blocks.append(block_text)

    return all_blocks

# --- æå– JSON ---
def extract_json(text: str) -> List[dict]:
    """
    æå–JSONï¼Œæ”¯æŒå¤šç§æƒ…å†µï¼š
    1. JSONæ•°ç»„ [{"question": "...", "answer": "..."}, ...]
    2. å•ä¸ªJSONå¯¹è±¡ {"question": "...", "answer": "..."}
    3. å¤šä¸ªç‹¬ç«‹çš„JSONå¯¹è±¡
    4. ç©ºæ•°ç»„ []
    """
    results = []
    
    try:
        # å…ˆå°è¯•æŸ¥æ‰¾```json```åŒ…è£¹çš„å†…å®¹
        json_blocks = re.findall(r'```json\s*(.*?)\s*```', text, re.DOTALL)
        
        if json_blocks:
            for json_block in json_blocks:
                results.extend(_parse_json_content(json_block))
        else:
            # å¦‚æœæ²¡æœ‰```json```åŒ…è£¹ï¼Œç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬
            results.extend(_parse_json_content(text))
    
    except Exception as e:
        error_logger.error(f"JSONæå–å¼‚å¸¸: {e}\nåŸå§‹å›å¤:\n{text}")
    
    # è¿‡æ»¤æœ‰æ•ˆçš„é—®ç­”å¯¹
    valid_results = []
    for data in results:
        if (isinstance(data, dict) and 
            "question" in data and "answer" in data and
            data.get("question") and data.get("answer") and
            str(data.get("question", "")).strip() and 
            str(data.get("answer", "")).strip()):
            valid_results.append(data)
    
    return valid_results

def _parse_json_content(content: str) -> List[dict]:
    """è§£æJSONå†…å®¹ï¼Œä¼˜å…ˆå¤„ç†æ•°ç»„æ ¼å¼ï¼Œç„¶åå¤„ç†å•ä¸ªå¯¹è±¡å’Œå¤šä¸ªå¯¹è±¡"""
    results = []
    content = content.strip()
    
    if not content:
        return results
    
    try:
        # é¦–å…ˆå°è¯•è§£æä¸ºJSONï¼ˆå¯èƒ½æ˜¯æ•°ç»„æˆ–å•ä¸ªå¯¹è±¡ï¼‰
        data = json.loads(content)
        if isinstance(data, list):
            # å¦‚æœæ˜¯æ•°ç»„ï¼Œç›´æ¥æ‰©å±•åˆ°ç»“æœä¸­
            for item in data:
                if isinstance(item, dict):
                    results.append(item)
        elif isinstance(data, dict):
            # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            results.append(data)
        return results
        
    except json.JSONDecodeError:
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•åˆ†ç¦»å¤šä¸ªJSONå¯¹è±¡
        try:
            # æŸ¥æ‰¾æ‰€æœ‰çš„JSONå¯¹è±¡
            json_objects = []
            brace_count = 0
            start_pos = -1
            
            for i, char in enumerate(content):
                if char == '{':
                    if brace_count == 0:
                        start_pos = i
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0 and start_pos != -1:
                        json_str = content[start_pos:i+1]
                        json_objects.append(json_str)
                        start_pos = -1
            
            # è§£ææ¯ä¸ªJSONå¯¹è±¡
            for json_str in json_objects:
                try:
                    data = json.loads(json_str)
                    if isinstance(data, dict):
                        results.append(data)
                except json.JSONDecodeError:
                    continue
                    
        except Exception as e:
            error_logger.error(f"JSONå¯¹è±¡åˆ†ç¦»å¤±è´¥: {e}\nå†…å®¹:\n{content}")
    
    return results

# --- è°ƒç”¨ Ollama ---
def call_ollama(client: Client, model_name: str, prompt_text: str) -> Optional[str]:
    try:
        response = client.chat(
            model=model_name,
            messages=[{"role": "user", "content": prompt_text}],
            options={"temperature": 0.1}
        )
        return response["message"]["content"]
    except Exception as e:
        logging.error(f"Ollama API è°ƒç”¨å¤±è´¥: {e}")
        return None

# --- ä¸»å¤„ç†æµç¨‹ ---
def process(pdf_path, output_path, model_name, ratio, max_size, min_size, enable_filter):
    print(f"ğŸ” å¼€å§‹å¤„ç†æ–‡ä»¶: {pdf_path}")
    raw_text = extract_text_from_pdf(pdf_path)
    print(f"ğŸ“„ æ–‡æœ¬æ€»é•¿åº¦: {len(raw_text)} å­—ç¬¦")

    print("âœ‚ï¸ åˆ†å—ä¸­...")
    blocks = create_hybrid_blocks(raw_text, max_size, min_size)
    print(f"âœ… å®Œæˆåˆ†å—ï¼Œå…± {len(blocks)} å—")

    if enable_filter:
        blocks = [b for b in blocks if block_has_qa(b)]
        print(f"âš¡ è¿‡æ»¤åå‰©ä½™å«é—®ç­”å—: {len(blocks)}")

    if ratio < 1.0:
        sample_size = int(len(blocks) * ratio)
        blocks = blocks[:max(sample_size, 1)]
        print(f"âš¡ é¡ºåºé‡‡æ · {len(blocks)} å— (æ¯”ä¾‹: {ratio:.0%})")

    if not blocks:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆå—ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    with open(output_path, "w", encoding="utf-8") as f:
        pass

    client = Client(host='http://localhost:11434')
    success_count = 0

    for block in tqdm(blocks, desc="ç”Ÿæˆé—®ç­”å¯¹ä¸­"):
        # é¢„å¤„ç†æ–‡æœ¬
        processed_block = preprocess_qa_text(block)
        prompt = BASE_PROMPT_CHAT + "\n\n" + processed_block

        reply = call_ollama(client, model_name, prompt)

        if reply is None:
            error_logger.error(f"APIè°ƒç”¨å¤±è´¥æˆ–æ— è¿”å›å†…å®¹ã€‚Block:\n{block}")
            continue

        data = extract_json(reply)

        if data:
            for i, d in enumerate(data):
                # å®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿questionå’Œanswerä¸ä¸ºNone
                if not d.get("question") or not d.get("answer"):
                    continue
                    
                clean_q = clean_question_text(d["question"])  # å·²ä¿®æ”¹
                final_data = {
                    "question": clean_q,
                    "answer": d["answer"],
                    "source_text": block
                }
                with open(output_path, "a", encoding="utf-8") as f:
                    f.write(json.dumps(final_data, ensure_ascii=False) + "\n")
                
                # è®°å½•æˆåŠŸæå–çš„é—®ç­”å¯¹
                success_log_content = f"""æˆåŠŸæå–é—®ç­”å¯¹ #{success_count + i + 1}:

é—®é¢˜: {d["question"]}

ç­”æ¡ˆ: {d["answer"]}

åŸæ–‡å—:
{block}

{'='*80}"""
                success_logger.info(success_log_content)
            
            success_count += len(data)
            print(f"âœ… ä»å½“å‰å—æå–äº† {len(data)} ä¸ªé—®ç­”å¯¹")
        else:
            print(f"âŒ å½“å‰å—æœªæå–åˆ°é—®ç­”å¯¹")
            error_logger.error(f"æ— æ³•æå–æœ‰æ•ˆé—®ç­”ã€‚æ¨¡å‹å›å¤: {reply}\n\nBlock:\n{block}")

    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼å…±å†™å…¥ {success_count} ä¸ªé—®ç­”å¯¹è‡³ {output_path}")
    print("é”™è¯¯ä¿¡æ¯å·²è®°å½•åˆ° extraction_errors_final.log æ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹ã€‚")

# --- å…¥å£ ---
if __name__ == "__main__":
    if "/path/to/your/file.pdf" in PDF_PATH:
        print("âŒ é”™è¯¯ï¼šè¯·å…ˆä¿®æ”¹ PDF_PATH ä¸ºä½ çš„ PDF æ–‡ä»¶è·¯å¾„ã€‚")
    else:
        process(
            pdf_path=PDF_PATH,
            output_path=OUTPUT_PATH,
            model_name=MODEL_NAME,
            ratio=EXTRACT_RATIO,
            max_size=MAX_BLOCK_SIZE,
            min_size=MIN_BLOCK_SIZE,
            enable_filter=ENABLE_QA_FILTER
        )