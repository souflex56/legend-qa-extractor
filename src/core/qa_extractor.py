"""Q&A extraction module for processing and extracting question-answer pairs."""

import json
import re
from typing import List, Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)


class QAExtractor:
    """Handles extraction and processing of Q&A pairs from LLM responses."""
    
    def __init__(self, max_prompt_tokens: int = 6000):
        self.logger = logger
        self.max_prompt_tokens = max_prompt_tokens  # ç•™ä¸€äº›ä½™é‡ç»™æ¨¡å‹
        
        # ç²¾ç®€ç‰ˆåŸºç¡€promptï¼Œä¿ç•™æ ¸å¿ƒåŠŸèƒ½ä½†å¤§å¹…ç¼©çŸ­
        self.compact_prompt = """ä½ æ˜¯ä¸­æ–‡é—®ç­”å¯¹æå–ä¸“å®¶ï¼Œä»åŸæ–‡æå–æ®µæ°¸å¹³çš„æ‰€æœ‰çœŸå®é—®ç­”å¯¹ã€‚

ã€æ ¸å¿ƒåŸåˆ™ã€‘å¿…é¡»æœ‰çœŸå®å¤–éƒ¨æé—®ï¼ˆç½‘å‹/ä¸»æŒäºº/å¼•ç”¨è§‚ç‚¹ï¼‰å¼•å‘æ®µæ°¸å¹³å›åº”ï¼Œæ¯ä¸ªé—®é¢˜å¯¹åº”ä¸€ä¸ªå®Œæ•´å›ç­”ï¼ˆå«è¿ç»­è¡¥å……ï¼‰ï¼Œç¦æ­¢æå–ä¿®è¾æ€§è‡ªé—®å¥ã€‚

ã€æå–æµç¨‹ã€‘æ‰¾åˆ°çœŸå®å¤–éƒ¨é—®é¢˜ â†’ åŒ¹é…å®Œæ•´æ®µæ°¸å¹³å›ç­” â†’ ä¸¥æ ¼éªŒè¯é…å¯¹

ã€ä¸¥ç¦ã€‘æ®µæ°¸å¹³é˜è¿°ä¸­çš„"ä»€ä¹ˆæ˜¯XXï¼Ÿ""å¾ˆéš¾å—ï¼Ÿ"ç­‰ä¿®è¾é—®å¥ã€é—®ç­”å†…å®¹ç›¸åŒ/é¢ å€’ã€æ— å¤–éƒ¨å¼•å‘å°±è¾“å‡º

è¾“å‡ºæ ¼å¼ï¼šJSONæ•°ç»„ [{"question": "å¤–éƒ¨é—®é¢˜", "answer": "æ®µæ°¸å¹³å®Œæ•´å›ç­”"}]

åŸæ–‡ï¼š"""
        
        # å®Œæ•´ç‰ˆåŸºç¡€promptï¼ˆå½“æœ‰å……è¶³ç©ºé—´æ—¶ä½¿ç”¨ï¼‰
        self.full_prompt = """ä½ æ˜¯ä¸“ä¸šä¸­æ–‡é—®ç­”å¯¹æå–ä¸“å®¶ï¼Œä»åŸæ–‡æå–æ®µæ°¸å¹³çš„æ‰€æœ‰æœ‰æ•ˆé—®ç­”å¯¹ã€‚

ğŸ¯ ã€æ ¸å¿ƒåŸåˆ™ã€‘
â€¢ å¿…é¡»å­˜åœ¨çœŸå®å¤–éƒ¨æé—®ï¼ˆç½‘å‹ã€ä¸»æŒäººã€å¼•ç”¨è§‚ç‚¹ç­‰ï¼‰å¼•å‘æ®µæ°¸å¹³å›åº”
â€¢ æ¯ä¸ªå¤–éƒ¨é—®é¢˜åªå¯¹åº”ä¸€ä¸ªå®Œæ•´åˆå¹¶å›ç­”ï¼ˆåŒ…å«æ‰€æœ‰åç»­è¡¥å……ç‰‡æ®µï¼‰
â€¢ ç»å¯¹ç¦æ­¢æå–æ®µæ°¸å¹³é˜è¿°ä¸­çš„ä¿®è¾æ€§è‡ªé—®å¥

ğŸ“‹ ã€æå–æµç¨‹ã€‘
1ï¸âƒ£ **é—®é¢˜è¯†åˆ«**ï¼šæ˜ç¡®æ ‡è¯†ï¼ˆç½‘å‹ï¼šã€é—®ï¼šï¼‰æˆ–å¼•ç”¨è§‚ç‚¹ï¼ˆæœ‰äººè¯´ã€æ–‡ç« å¼•ç”¨ï¼‰
2ï¸âƒ£ **å›ç­”åŒ¹é…**ï¼šæ®µæ°¸å¹³çš„å®Œæ•´è¿ç»­å›åº”ï¼ˆå«æ‰€æœ‰ç›¸å…³è¡¥å……ï¼‰
3ï¸âƒ£ **é…å¯¹éªŒè¯**ï¼šé—®é¢˜ä¸å›ç­”é€»è¾‘å¯¹åº”ï¼Œæ— å†…å®¹é‡å¤/é¢ å€’

ğŸ”§ ã€è¾¹ç•Œå¤„ç†ã€‘
â€¢ **åŒä¸€é—®é¢˜çš„ç¦»æ•£å›ç­”**ï¼šåˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´answer
â€¢ **æ–°é—®é¢˜åˆ¤æ–­**ï¼šå‡ºç°æ–°æé—®è€…æˆ–è¯é¢˜å®è´¨è½¬æ¢
â€¢ **ä¿®è¾æ€§é—®å¥**ï¼šæ®µæ°¸å¹³è®ºè¿°ä¸­çš„"ä»€ä¹ˆæ˜¯XXï¼Ÿ""å¾ˆéš¾å—ï¼Ÿ"ç­‰ä¸æå–

âœ… **æ ¸å¿ƒç¤ºä¾‹**
```
ç½‘å‹ï¼šä»€ä¹ˆæ˜¯stop doing listï¼Ÿ
æ®µæ°¸å¹³ï¼šæ‰€è°“è¦åšå¯¹çš„äº‹æƒ…å®é™…ä¸Šæ˜¯é€šè¿‡ä¸åšä¸å¯¹çš„äº‹æƒ…æ¥å®ç°çš„ã€‚

æœ‰äººè®¤ä¸ºä»·å€¼æŠ•èµ„å·²ç»è¿‡æ—¶äº†ã€‚
æˆ‘ä¸è¿™ä¹ˆè®¤ä¸ºã€‚ä»·å€¼æŠ•èµ„æ°¸è¿œä¸ä¼šè¿‡æ—¶ï¼Œå› ä¸ºå®ƒçš„æœ¬è´¨æ˜¯ä¹°ä¼˜ç§€çš„å…¬å¸ã€‚

ä¸»æŒäººï¼šæŠ•èµ„ä¸­æœ€éš¾çš„æ˜¯ä»€ä¹ˆï¼Ÿ
æ®µæ°¸å¹³ï¼šæœ€éš¾çš„æ˜¯å…‹æœææƒ§å’Œè´ªå©ªã€‚è¿™æ˜¯äººæ€§ã€‚
ä¸»æŒäººï¼šè¿˜æœ‰å—ï¼Ÿ
æ®µæ°¸å¹³ï¼šè¿˜æœ‰å°±æ˜¯åšæŒä¸æ‡‚ä¸åšã€‚åªåœ¨è‡ªå·±çš„èƒ½åŠ›åœˆå†…æ´»åŠ¨ã€‚
```

æ­£ç¡®è¾“å‡ºï¼š
```json
[
  {"question": "ä»€ä¹ˆæ˜¯stop doing listï¼Ÿ", "answer": "æ‰€è°“è¦åšå¯¹çš„äº‹æƒ…å®é™…ä¸Šæ˜¯é€šè¿‡ä¸åšä¸å¯¹çš„äº‹æƒ…æ¥å®ç°çš„ã€‚"},
  {"question": "æœ‰äººè®¤ä¸ºä»·å€¼æŠ•èµ„å·²ç»è¿‡æ—¶äº†ã€‚", "answer": "æˆ‘ä¸è¿™ä¹ˆè®¤ä¸ºã€‚ä»·å€¼æŠ•èµ„æ°¸è¿œä¸ä¼šè¿‡æ—¶ï¼Œå› ä¸ºå®ƒçš„æœ¬è´¨æ˜¯ä¹°ä¼˜ç§€çš„å…¬å¸ã€‚"},
  {"question": "æŠ•èµ„ä¸­æœ€éš¾çš„æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "æœ€éš¾çš„æ˜¯å…‹æœææƒ§å’Œè´ªå©ªã€‚è¿™æ˜¯äººæ€§ã€‚è¿˜æœ‰å°±æ˜¯åšæŒä¸æ‡‚ä¸åšã€‚åªåœ¨è‡ªå·±çš„èƒ½åŠ›åœˆå†…æ´»åŠ¨ã€‚"}
]
```

âŒ **é›†ä¸­é”™è¯¯é˜²èŒƒ**
â€¢ æŠŠæ®µæ°¸å¹³çš„ä¿®è¾é—®å¥å½“å¤–éƒ¨é—®é¢˜ï¼ˆå¦‚"ä»·å€¼æŠ•èµ„çš„æ ¸å¿ƒæ˜¯ä»€ä¹ˆï¼Ÿå°±æ˜¯ä¹°ä¼˜ç§€å…¬å¸"ä¸­çš„é—®å¥ï¼‰
â€¢ é—®é¢˜å’Œç­”æ¡ˆå†…å®¹ç›¸åŒæˆ–é€»è¾‘é¢ å€’
â€¢ æ‹†åˆ†å±äºåŒä¸€é—®é¢˜çš„è¿ç»­å›ç­”ç‰‡æ®µ
â€¢ æ— å¤–éƒ¨é—®é¢˜å¼•å‘å°±è¾“å‡ºé—®ç­”å¯¹

ğŸ” è¯·ä»”ç»†åˆ†æä»¥ä¸‹åŸæ–‡ï¼Œæå–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„é—®ç­”å¯¹ï¼š
"""
    
    def estimate_token_count(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆä¸­æ–‡çº¦1.5å€å­—ç¬¦æ•°ï¼‰"""
        # ä¸­æ–‡å­—ç¬¦å’Œtokenæ¯”ä¾‹çº¦1:1.5ï¼Œè‹±æ–‡çº¦1:0.75ï¼Œå–ä¿å®ˆä¼°è®¡
        chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
        other_chars = len(text) - chinese_chars
        return int(chinese_chars * 1.5 + other_chars * 0.75)
    
    def create_prompt(self, text_block: str, sliding_context: str = "", block_anchor: str = "") -> str:
        """åˆ›å»ºå®Œæ•´çš„LLMæç¤ºï¼Œæ”¯æŒæ™ºèƒ½é•¿åº¦ç®¡ç†ã€‚
        
        Args:
            text_block: éœ€è¦æå–é—®ç­”å¯¹çš„æ–‡æœ¬å†…å®¹
            sliding_context: å‰ä¸€ä¸ªç›¸å…³æ–‡æœ¬å—çš„æœ«å°¾éƒ¨åˆ†ï¼Œç”¨äºæä¾›é¢å¤–ä¸Šä¸‹æ–‡
            block_anchor: å½“å‰æ–‡æœ¬å—çš„æ ¸å¿ƒä¸»é¢˜æˆ–å…³é”®è¯
            
        Returns:
            å®Œæ•´çš„æç¤ºæ–‡æœ¬
        """
        # æ„å»ºä¸Šä¸‹æ–‡éƒ¨åˆ†
        context_section = ""
        if sliding_context:
            context_section += f"\n\nä¸Šä¸‹æ–‡ï¼š{sliding_context[:200]}...\n"  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        
        if block_anchor:
            context_section += f"\nä¸»é¢˜ï¼š{block_anchor}\n"
        
        # å°è¯•ä½¿ç”¨å®Œæ•´prompt
        full_prompt_text = f"{self.full_prompt}{context_section}\n\n{text_block}"
        full_tokens = self.estimate_token_count(full_prompt_text)
        
        # å¦‚æœå®Œæ•´promptä¸è¶…é™ï¼Œä½¿ç”¨å®Œæ•´ç‰ˆ
        if full_tokens <= self.max_prompt_tokens:
            self.logger.debug(f"Using full prompt, estimated tokens: {full_tokens}")
            return full_prompt_text
        
        # å¦åˆ™ä½¿ç”¨ç²¾ç®€ç‰ˆprompt
        compact_prompt_text = f"{self.compact_prompt}{context_section}\n\n{text_block}"
        compact_tokens = self.estimate_token_count(compact_prompt_text)
        
        # å¦‚æœç²¾ç®€ç‰ˆä»è¶…é™ï¼Œéœ€è¦æˆªæ–­æ–‡æœ¬å—
        if compact_tokens > self.max_prompt_tokens:
            # è®¡ç®—å¯ç”¨äºæ–‡æœ¬å—çš„tokenæ•°
            base_tokens = self.estimate_token_count(f"{self.compact_prompt}{context_section}")
            available_tokens = self.max_prompt_tokens - base_tokens - 100  # ç•™100tokenä½™é‡
            
            # ä¼°ç®—å¯å®¹çº³çš„å­—ç¬¦æ•°
            available_chars = int(available_tokens / 1.5)  # ä¿å®ˆä¼°è®¡
            
            if available_chars > 100:  # ç¡®ä¿æœ‰æœ€å°çš„æ–‡æœ¬é‡
                truncated_block = self._smart_truncate_text(text_block, available_chars)
                compact_prompt_text = f"{self.compact_prompt}{context_section}\n\n{truncated_block}"
                self.logger.warning(f"Text block truncated to {len(truncated_block)} chars due to token limit")
            else:
                # å¦‚æœè¿æœ€å°æ–‡æœ¬éƒ½æ”¾ä¸ä¸‹ï¼Œå»æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯
                compact_prompt_text = f"{self.compact_prompt}\n\n{text_block[:available_chars]}"
                self.logger.warning(f"Context removed due to token limit, text truncated to {available_chars} chars")
        
        final_tokens = self.estimate_token_count(compact_prompt_text)
        self.logger.debug(f"Using compact prompt, estimated tokens: {final_tokens}")
        return compact_prompt_text
    
    def _smart_truncate_text(self, text: str, max_chars: int) -> str:
        """æ™ºèƒ½æˆªæ–­æ–‡æœ¬ï¼Œå°½é‡ä¿æŒå®Œæ•´æ€§"""
        if len(text) <= max_chars:
            return text
        
        # å°è¯•æŒ‰æ®µè½æˆªæ–­
        paragraphs = text.split('\n\n')
        result = ""
        for para in paragraphs:
            if len(result) + len(para) + 2 <= max_chars:  # +2 for \n\n
                result += para + "\n\n" if result else para
            else:
                break
        
        # å¦‚æœæŒ‰æ®µè½æˆªæ–­åå¤ªçŸ­ï¼Œå°è¯•æŒ‰å¥å­æˆªæ–­
        if len(result) < max_chars * 0.7:  # å¦‚æœæˆªæ–­åå°‘äº70%ï¼Œå°è¯•å¥å­çº§æˆªæ–­
            sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›])', text)
            result = ""
            for sentence in sentences:
                if len(result) + len(sentence) <= max_chars:
                    result += sentence
                else:
                    break
        
        # æœ€åç¡®ä¿ä¸è¶…é•¿
        if len(result) > max_chars:
            result = result[:max_chars-3] + "..."
        
        return result.strip()
    
    def extract_json(self, text: str) -> List[Dict[str, Any]]:
        """Extract JSON data from LLM response.
        
        Args:
            text: LLM response text containing JSON
            
        Returns:
            List of Q&A pair dictionaries
        """
        results = []
        
        try:
            # Try to find JSON blocks wrapped in ```json```
            json_blocks = re.findall(r'```json\s*(.*?)\s*```', text, re.DOTALL)
            
            if json_blocks:
                for json_block in json_blocks:
                    results.extend(self._parse_json_content(json_block))
            else:
                # If no ```json``` wrapper, parse the entire text
                results.extend(self._parse_json_content(text))
        
        except Exception as e:
            self.logger.error(f"JSON extraction error: {e}\nOriginal response:\n{text}")
        
        # Filter valid Q&A pairs
        valid_results = []
        for data in results:
            if self._is_valid_qa_pair(data):
                valid_results.append(data)
        
        return valid_results
    
    def _parse_json_content(self, content: str) -> List[Dict[str, Any]]:
        """Parse JSON content, handling arrays, single objects, and multiple objects.
        
        Args:
            content: JSON content string to parse
            
        Returns:
            List of parsed JSON objects
        """
        results = []
        content = content.strip()
        
        if not content:
            return results
        
        try:
            # First try to parse as JSON (could be array or single object)
            data = json.loads(content)
            if isinstance(data, list):
                # If it's an array, extend results
                for item in data:
                    if isinstance(item, dict):
                        results.append(item)
            elif isinstance(data, dict):
                # If it's a single object, add to results
                results.append(data)
            return results
            
        except json.JSONDecodeError:
            # If JSON parsing fails, try to separate multiple JSON objects
            try:
                json_objects = self._extract_json_objects(content)
                for json_str in json_objects:
                    try:
                        data = json.loads(json_str)
                        if isinstance(data, dict):
                            results.append(data)
                    except json.JSONDecodeError:
                        continue
                        
            except Exception as e:
                self.logger.error(f"JSON object separation failed: {e}\nContent:\n{content}")
        
        return results
    
    def _extract_json_objects(self, content: str) -> List[str]:
        """Extract individual JSON objects from text containing multiple objects.
        
        Args:
            content: Text content containing JSON objects
            
        Returns:
            List of JSON object strings
        """
        json_objects = []
        brace_count = 0
        start_pos = -1
        
        for i, char in enumerate(content):
            if char == '{':
                if brace_count == 0:
                    start_pos = i
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0 and start_pos != -1:
                    json_str = content[start_pos:i+1]
                    json_objects.append(json_str)
                    start_pos = -1
        
        return json_objects
    
    def _is_valid_qa_pair(self, data: Any) -> bool:
        """Check if the data is a valid Q&A pair.
        
        Args:
            data: Data object to validate
            
        Returns:
            True if valid Q&A pair, False otherwise
        """
        return (isinstance(data, dict) and 
                "question" in data and "answer" in data and
                data.get("question") and data.get("answer") and
                str(data.get("question", "")).strip() and 
                str(data.get("answer", "")).strip())
    
    def process_qa_pairs(self, qa_pairs: List[Dict[str, Any]], 
                        source_text: str, 
                        text_processor) -> List[Dict[str, Any]]:
        """Process extracted Q&A pairs with cleaning and formatting.
        
        Args:
            qa_pairs: List of raw Q&A pairs
            source_text: Original source text
            text_processor: TextProcessor instance for cleaning
            
        Returns:
            List of processed Q&A pairs
        """
        processed_pairs = []
        
        for qa_pair in qa_pairs:
            if not self._is_valid_qa_pair(qa_pair):
                continue
            
            # Clean question text
            clean_question = text_processor.clean_question_text(qa_pair["question"])
            
            # Create final Q&A pair
            final_pair = {
                "question": clean_question,
                "answer": qa_pair["answer"],
                "source_text": source_text
            }
            
            processed_pairs.append(final_pair)
        
        return processed_pairs
    
    def validate_extraction_quality(self, original_text: str, 
                                   qa_pairs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Validate the quality of extracted Q&A pairs.
        
        Args:
            original_text: Original text that was processed
            qa_pairs: Extracted Q&A pairs
            
        Returns:
            Quality metrics dictionary
        """
        metrics = {
            'total_pairs': len(qa_pairs),
            'avg_question_length': 0,
            'avg_answer_length': 0,
            'has_duplicates': False,
            'quality_score': 0.0
        }
        
        if not qa_pairs:
            return metrics
        
        # Calculate average lengths
        question_lengths = [len(pair['question']) for pair in qa_pairs]
        answer_lengths = [len(pair['answer']) for pair in qa_pairs]
        
        metrics['avg_question_length'] = sum(question_lengths) / len(question_lengths)
        metrics['avg_answer_length'] = sum(answer_lengths) / len(answer_lengths)
        
        # Check for duplicates
        questions = [pair['question'] for pair in qa_pairs]
        metrics['has_duplicates'] = len(questions) != len(set(questions))
        
        # Calculate quality score (simple heuristic)
        score = 0.0
        if metrics['avg_question_length'] > 5:
            score += 0.3
        if metrics['avg_answer_length'] > 10:
            score += 0.3
        if not metrics['has_duplicates']:
            score += 0.2
        if metrics['total_pairs'] > 0:
            score += 0.2
        
        metrics['quality_score'] = score
        
        return metrics 