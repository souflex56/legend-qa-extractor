# src/core/smart_block_processor.py

import re
import logging
from typing import List, Dict, Any

from .text_processor import TextProcessor
from .llm_client import LLMClient

logger = logging.getLogger(__name__)


class SmartBlockProcessor:
    """
    负责智能文本分块 (V1 精简版)。
    核心策略：结构化分块 -> 智能合并 -> LLM生成Anchor -> 添加滑动上下文
    """
    
    def __init__(self,
                 text_processor: TextProcessor,
                 llm_client: LLMClient,
                 max_block_size: int = 2048,
                 min_block_size: int = 200,
                 qa_allowance_ratio: float = 1.2,
                 enable_sliding_context: bool = False,
                 enable_llm_anchor: bool = False,
                 anchor_keywords_count: int = 3):
        
        self.text_processor = text_processor
        self.llm_client = llm_client
        self.logger = logger
        
        # 分块参数
        self.max_block_size = max_block_size
        self.min_block_size = min_block_size
        self.qa_allowance_ratio = qa_allowance_ratio
        
        # 功能开关
        self.enable_sliding_context = enable_sliding_context
        self.enable_llm_anchor = enable_llm_anchor
        self.anchor_keywords_count = anchor_keywords_count
        
        # 复用 TextProcessor 中的模式定义
        self.direct_question_patterns = [
            r"网友[：:]",
            r"问[：:]",
            r"问题[：:]", 
            r"提问[：:]",
            r"主持人[：:]",
            r"观众[：:]",
            r"Q[：:]"
        ]
        
        self.indirect_question_patterns = [
            r"文章引用[：:]",
            r"引用[：:]",
            r"有人说",
            r"有人认为",
            r"有观点认为",
            r"据说",
            r"听说"
        ]
        
        self.answer_patterns = [
            r"段永平[：:]",
            r"段[：:]",
            r"大道[：:]"
        ]

    def process_document_into_blocks(self, text: str) -> List[Dict[str, Any]]:
        """
        主入口：综合分层策略处理文档并生成文本块。
        返回的每个块是字典，包含 'content' 和可选的 'anchor', 'sliding_context'。
        """
        if not text:
            return []

        # 1. 第一层：基于结构的硬分块
        self.logger.info("Step 1: Starting structural blocking...")
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        structural_blocks = self._hard_block_by_structure(paragraphs)
        self.logger.info(f"Step 1: Generated {len(structural_blocks)} initial structural blocks.")

        # 2. 第二层：智能合并
        self.logger.info("Step 2: Starting adaptive merging...")
        merged_blocks_content = self._adaptive_merge_blocks(structural_blocks)
        self.logger.info(f"Step 2: Adaptively merged into {len(merged_blocks_content)} blocks.")

        # 3. 第三层：后处理与质量保障
        self.logger.info("Step 3: Starting quality assurance filtering...")
        qa_filtered_blocks_content = self._post_process_blocks(merged_blocks_content)
        self.logger.info(f"Step 3: Quality-assured blocks count: {len(qa_filtered_blocks_content)}.")

        # 4. 第四层：为每个块生成元数据 (Anchor & Context)
        self.logger.info("Step 4: Starting metadata generation for all blocks...")
        final_output_blocks: List[Dict[str, Any]] = []
        
        # 只为较重要的块生成LLM锚点，以减少调用次数
        llm_anchor_candidates = self._select_anchor_candidates(qa_filtered_blocks_content) if self.enable_llm_anchor else set()
        
        for i, block_content in enumerate(qa_filtered_blocks_content):
            block_data = {"content": block_content}

            # 有选择地使用 LLM 生成 Anchor
            if self.enable_llm_anchor and i in llm_anchor_candidates:
                anchor = self._generate_anchor_with_llm(block_content)
                if anchor:
                    block_data["anchor"] = anchor
                    self.logger.debug(f"Block {i} anchor generated by LLM: {anchor}")

            # 添加柔性跨块引用 (Sliding Context)
            if self.enable_sliding_context and i > 0:
                prev_block_content = qa_filtered_blocks_content[i-1]
                sliding_context = self._get_sliding_context(prev_block_content)
                if sliding_context:
                    block_data["sliding_context"] = sliding_context
                    self.logger.debug(f"Block {i} has sliding context.")
            
            final_output_blocks.append(block_data)
        
        self.logger.info("Step 4: Metadata generation complete.")
        return final_output_blocks

    def _hard_block_by_structure(self, paragraphs: List[str]) -> List[str]:
        """基于结构的硬分块。利用段落、标题和Q&A标记。"""
        title_patterns = [
            r"^\s*(#+)\s*[\u4e00-\u9fa5A-Za-z0-9\s\-_]+", 
            r"^\s*(\d+\.?)+\s+[\u4e00-\u9fa5A-Za-z0-9\s\-_]+",
            r"^\s*[\u4e00-\u9fa5A-Za-z0-9\s\-_]+\s*[:：]\s*$",
            r'^\s*[【〖《"][^【〖《"]+[】〗》"]\s*$',
        ]
        
        qa_start_patterns = self.direct_question_patterns + self.indirect_question_patterns
        
        blocks, current_block_paras = [], []
        
        for para in paragraphs:
            is_new_hard_boundary = any(re.search(p, para) for p in title_patterns)
            
            if not is_new_hard_boundary and any(re.search(p, para) for p in qa_start_patterns):
                if current_block_paras and not self.text_processor.block_has_qa("\n\n".join(current_block_paras)):
                    is_new_hard_boundary = True
                    
            if is_new_hard_boundary and current_block_paras:
                blocks.append("\n\n".join(current_block_paras))
                current_block_paras = []
                
            current_block_paras.append(para)
            
        if current_block_paras:
            blocks.append("\n\n".join(current_block_paras))
            
        return blocks

    def _adaptive_merge_blocks(self, blocks_content: List[str]) -> List[str]:
        """智能合并。将小块合并到目标长度范围。"""
        merged_blocks, current_block_parts, current_block_len = [], [], 0
        
        for block_text in blocks_content:
            is_qa_candidate = self.text_processor.block_has_qa(block_text)
            effective_max_size = self.max_block_size * (self.qa_allowance_ratio if is_qa_candidate else 1.0)
            
            if current_block_len > 0 and current_block_len + len(block_text) > effective_max_size:
                merged_blocks.append("\n\n".join(current_block_parts))
                current_block_parts = [block_text]
                current_block_len = len(block_text)
            else:
                current_block_parts.append(block_text)
                current_block_len += len(block_text)
                
        if current_block_parts:
            merged_blocks.append("\n\n".join(current_block_parts))
            
        # 重新合并过小的块
        final_merged = []
        for block in merged_blocks:
            if len(block) < self.min_block_size and final_merged:
                # 尝试与上一个块合并
                last_block = final_merged[-1]
                if len(last_block) + len(block) <= self.max_block_size * 1.5:
                    final_merged[-1] = last_block + "\n\n" + block
                else:
                    final_merged.append(block)
            else:
                final_merged.append(block)
        
        # **新增：强制分割超长块**
        force_split_blocks = []
        for block in final_merged:
            if len(block) > self.max_block_size:
                self.logger.warning(f"Force splitting oversized block ({len(block)} chars) into smaller pieces")
                split_pieces = self._force_split_oversized_block(block)
                force_split_blocks.extend(split_pieces)
            else:
                force_split_blocks.append(block)
                
        return force_split_blocks
    
    def _force_split_oversized_block(self, oversized_block: str) -> List[str]:
        """
        强制分割超长文本块，采用多级分割策略：
        1. 按段落分割
        2. 按句子分割  
        3. 按固定长度分割（最后保障）
        """
        if len(oversized_block) <= self.max_block_size:
            return [oversized_block]
        
        split_pieces = []
        
        # 第一级：按段落分割
        paragraphs = [p.strip() for p in oversized_block.split('\n\n') if p.strip()]
        
        current_piece = ""
        for para in paragraphs:
            # 如果单个段落就超长，需要进一步分割
            if len(para) > self.max_block_size:
                # 先保存当前积累的内容
                if current_piece.strip():
                    split_pieces.append(current_piece.strip())
                    current_piece = ""
                
                # 分割超长段落
                para_pieces = self._split_oversized_paragraph(para)
                split_pieces.extend(para_pieces)
            else:
                # 检查是否可以加入当前piece
                if current_piece and len(current_piece) + len(para) + 2 > self.max_block_size:
                    split_pieces.append(current_piece.strip())
                    current_piece = para
                else:
                    current_piece = current_piece + "\n\n" + para if current_piece else para
        
        # 保存最后的piece
        if current_piece.strip():
            split_pieces.append(current_piece.strip())
        
        # 确保所有piece都不超长（最后保障）
        final_pieces = []
        for piece in split_pieces:
            if len(piece) > self.max_block_size:
                # 如果还是超长，按句子分割
                sentence_pieces = self._split_by_sentences(piece)
                final_pieces.extend(sentence_pieces)
            else:
                final_pieces.append(piece)
        
        self.logger.info(f"Split oversized block into {len(final_pieces)} pieces")
        return final_pieces
    
    def _split_oversized_paragraph(self, paragraph: str) -> List[str]:
        """分割超长段落，按句子边界分割"""
        if len(paragraph) <= self.max_block_size:
            return [paragraph]
        
        return self._split_by_sentences(paragraph)
    
    def _split_by_sentences(self, text: str) -> List[str]:
        """按句子分割文本"""
        # 中文句子分割模式
        sentences = re.split(r'(?<=[。！？；])\s*', text)
        pieces = []
        current_piece = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # 如果单个句子就超长，按固定长度分割
            if len(sentence) > self.max_block_size:
                if current_piece.strip():
                    pieces.append(current_piece.strip())
                    current_piece = ""
                
                # 按固定长度分割
                char_pieces = self._split_by_chars(sentence)
                pieces.extend(char_pieces)
            else:
                # 检查是否可以加入当前piece
                if current_piece and len(current_piece) + len(sentence) > self.max_block_size:
                    pieces.append(current_piece.strip())
                    current_piece = sentence
                else:
                    current_piece = current_piece + sentence if current_piece else sentence
        
        # 保存最后的piece
        if current_piece.strip():
            pieces.append(current_piece.strip())
        
        return pieces
    
    def _split_by_chars(self, text: str) -> List[str]:
        """按固定字符长度分割文本（最后保障）"""
        pieces = []
        for i in range(0, len(text), self.max_block_size):
            piece = text[i:i + self.max_block_size]
            if piece.strip():
                pieces.append(piece.strip())
        return pieces
    
    def _post_process_blocks(self, blocks: List[str]) -> List[str]:
        """后处理与质量保障。复用 TextProcessor 的质量验证功能。"""
        return [block for block in blocks if self.text_processor.validate_text_quality(block)]

    def _generate_anchor_with_llm(self, block_content: str) -> str:
        """使用LLM为文本块提取核心关键词作为Anchor。"""
        prompt = f"""请为以下文本内容提取 {self.anchor_keywords_count} 个核心关键词。
请只返回关键词本身，并用逗号分隔，不要添加任何其他解释或前缀。

文本内容：
---
{block_content[:1500]} 
---
关键词：
"""
        try:
            # 建议使用低温以保证输出稳定性
            keywords = self.llm_client.call_ollama(prompt, temperature=0.0)
            if keywords:
                cleaned_keywords = keywords.strip().replace("关键词：", "").replace("核心关键词：", "").strip()
                return cleaned_keywords
            return ""
        except Exception as e:
            self.logger.error(f"Failed to generate anchor with LLM: {e}")
            return ""

    def _get_sliding_context(self, prev_block_content: str, context_sentences: int = 2) -> str:
        """实现柔性跨块引用。获取上一个块的末尾部分作为上下文。"""
        if not self.enable_sliding_context:
            return ""
            
        sentences = [s.strip() for s in re.split(r'(?<=[。？！；\.])\s*', prev_block_content) if s.strip()]
        return " ".join(sentences[-context_sentences:]) if sentences else ""

    def _select_anchor_candidates(self, blocks_content: List[str]) -> set:
        """智能选择需要生成LLM锚点的文本块，以减少不必要的LLM调用。"""
        candidates = set()
        
        # 策略1: 选择包含QA内容的块
        for i, block in enumerate(blocks_content):
            if self.text_processor.block_has_qa(block):
                candidates.add(i)
        
        # 策略2: 选择较长的块（可能包含更多信息）
        sorted_blocks = sorted(enumerate(blocks_content), key=lambda x: len(x[1]), reverse=True)
        # 选择前30%最长的块，但不超过20个
        top_count = min(max(1, len(blocks_content) // 3), 20)
        for i, _ in sorted_blocks[:top_count]:
            candidates.add(i)
        
        # 策略3: 如果候选块太少，至少选择几个分布均匀的块
        if len(candidates) < 3:
            step = max(1, len(blocks_content) // 3)
            for i in range(0, len(blocks_content), step):
                candidates.add(i)
        
        self.logger.info(f"Selected {len(candidates)} blocks (out of {len(blocks_content)}) for LLM anchor generation")
        return candidates 